{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-11T12:07:34.782475Z",
     "iopub.status.busy": "2024-12-11T12:07:34.781523Z",
     "iopub.status.idle": "2024-12-11T12:07:37.388939Z",
     "shell.execute_reply": "2024-12-11T12:07:37.386646Z"
    }
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fetch the top 5000 delegates data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-11T12:07:37.395725Z",
     "iopub.status.busy": "2024-12-11T12:07:37.394732Z",
     "iopub.status.idle": "2024-12-11T12:07:37.415715Z",
     "shell.execute_reply": "2024-12-11T12:07:37.413528Z"
    }
   },
   "outputs": [],
   "source": [
    "# Function to fetch the top 5000 delegates  \n",
    "def fetch_top_5000_delegates():\n",
    "    \"\"\"\n",
    "    Fetches the top 5000 delegates using the skip parameter.\n",
    "    Converts the block timestamp and latest balance to readable formats.\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: A DataFrame containing delegate data.\n",
    "        filename\n",
    "    \"\"\"\n",
    "    # API endpoint\n",
    "    url = \"https://api.studio.thegraph.com/query/87007/op-delegates-list/version/latest\"\n",
    "\n",
    "    # GraphQL query template with a placeholder for skip value\n",
    "    query_template = \"\"\"\n",
    "    query MyQuery {{\n",
    "      delegates(orderBy: latestBalance, orderDirection: desc, first: 1000, skip: {skip_value}) {{\n",
    "        id\n",
    "        latestBalance\n",
    "      }}\n",
    "    }}\n",
    "    \"\"\"\n",
    "\n",
    "    def fetch_data(skip_value):\n",
    "        \"\"\"\n",
    "        Executes the GraphQL query with the given skip value and returns the fetched data.\n",
    "        \"\"\"\n",
    "        query = query_template.format(skip_value=skip_value)\n",
    "        response = requests.post(url, json={\"query\": query})\n",
    "        response.raise_for_status()  # Raise an error for HTTP issues\n",
    "\n",
    "        result = response.json()\n",
    "\n",
    "        # Handle GraphQL errors\n",
    "        if \"errors\" in result:\n",
    "            print(\"GraphQL Error:\", result[\"errors\"])\n",
    "            raise ValueError(\"GraphQL query failed.\")\n",
    "\n",
    "        return result.get(\"data\", {}).get(\"delegates\", [])\n",
    "\n",
    "    # Initialize variables for pagination\n",
    "    all_data = []  # To store all results\n",
    "    skip = 0       # Start with skip=0\n",
    "    batch_size = 1000\n",
    "\n",
    "    while len(all_data) < 5000:\n",
    "        try:\n",
    "            # Fetch data for the current skip value\n",
    "            data = fetch_data(skip)\n",
    "        except Exception as e:\n",
    "            print(f\"Error fetching data for skip={skip}: {e}\")\n",
    "            break\n",
    "\n",
    "        if not data:  # Exit if no more data is returned\n",
    "            break\n",
    "\n",
    "        all_data.extend(data)\n",
    "\n",
    "        # Increment skip by batch size\n",
    "        skip += batch_size\n",
    "\n",
    "        # Stop if we've fetched 5000 records\n",
    "        if len(all_data) >= 5000:\n",
    "            all_data = all_data[:5000]  # Trim excess records\n",
    "            break\n",
    "\n",
    "    if all_data:\n",
    "        # Convert the data to a DataFrame\n",
    "        df = pd.DataFrame(all_data)\n",
    "\n",
    "        # Rename columns to match the required names\n",
    "        df.rename(columns={\"id\": \"delegate\", \"latestBalance\": \"voting_power\"}, inplace=True)\n",
    "\n",
    "        # Convert latestBalance by dividing by 10^18\n",
    "        df['voting_power'] = df[\"voting_power\"].astype(float) / 10**18\n",
    "        \n",
    "        # Get the current date in the format %Y-%m-%d\n",
    "        current_date = datetime.now().strftime(\"%Y-%m-%d\")\n",
    "\n",
    "        # Save the dataframe to CSV with the current date as the filename\n",
    "        filepath = f\"./Data/{current_date}.csv\"\n",
    "        df.to_csv(filepath, index=False)\n",
    "        print(f\"Data saved to {filepath}\")\n",
    "\n",
    "        # Extract the filename from the path\n",
    "        filename = os.path.basename(filepath)\n",
    "\n",
    "        return df, filename\n",
    "    else:\n",
    "        print(\"No data fetched. Returning an empty DataFrame.\")\n",
    "        return pd.DataFrame(), None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add missing members in the delegate list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-11T12:07:37.421592Z",
     "iopub.status.busy": "2024-12-11T12:07:37.421592Z",
     "iopub.status.idle": "2024-12-11T12:07:37.429978Z",
     "shell.execute_reply": "2024-12-11T12:07:37.428958Z"
    }
   },
   "outputs": [],
   "source": [
    "# Helper function to add missing delegates\n",
    "def add_missing_delegates(data, new_addresses):\n",
    "    missing_addresses = set(new_addresses['address'].str.lower()) - set(data['delegate'].str.lower())\n",
    "    missing_df = pd.DataFrame({\n",
    "        'delegate': list(missing_addresses),\n",
    "        'voting_power': 0\n",
    "    })\n",
    "    data = pd.concat([data, missing_df], ignore_index=True)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert Dates to Datetime Format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-11T12:07:37.437144Z",
     "iopub.status.busy": "2024-12-11T12:07:37.436023Z",
     "iopub.status.idle": "2024-12-11T12:07:37.446798Z",
     "shell.execute_reply": "2024-12-11T12:07:37.445579Z"
    }
   },
   "outputs": [],
   "source": [
    "def convert_dates(df):\n",
    "    \"\"\"\n",
    "    Converts the 'start_date' and 'end_date' columns to datetime format for a given dataframe.\n",
    "\n",
    "    Parameters:\n",
    "        df (pd.DataFrame): The dataframe containing 'start_date' and 'end_date' columns.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: The dataframe with 'start_date' and 'end_date' converted to datetime format.\n",
    "    \"\"\"\n",
    "    if 'start_date' in df.columns and 'end_date' in df.columns:\n",
    "        try:\n",
    "            df.loc[:, 'start_date'] = pd.to_datetime(df['start_date'], dayfirst=True)\n",
    "            df.loc[:, 'end_date'] = pd.to_datetime(df['end_date'], dayfirst=True)\n",
    "        except Exception as e:\n",
    "            print(f\"Error converting dates: {e}\")\n",
    "    else:\n",
    "        print(\"Warning: 'start_date' or 'end_date' columns are missing in the dataframe.\")\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add Membership of Members"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-11T12:07:37.453648Z",
     "iopub.status.busy": "2024-12-11T12:07:37.452839Z",
     "iopub.status.idle": "2024-12-11T12:07:37.464058Z",
     "shell.execute_reply": "2024-12-11T12:07:37.463038Z"
    }
   },
   "outputs": [],
   "source": [
    "def add_membership_columns(data, councils, file_date):\n",
    "    for col_name, council_data in councils:\n",
    "        # Check if council is active during the given date\n",
    "        is_active = (file_date >= council_data['start_date'].iloc[0]) & (file_date <= council_data['end_date'].iloc[0])\n",
    "        if is_active:\n",
    "            # Add membership column for active councils\n",
    "            data[col_name] = data['delegate'].apply(\n",
    "                lambda x: 1 if x.lower() in council_data['address'].str.lower().values else 0   \n",
    "            )\n",
    "        else:\n",
    "            # Default to 0 if not active \n",
    "            data[col_name] = 0\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assign Voting Power"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-11T12:07:37.471101Z",
     "iopub.status.busy": "2024-12-11T12:07:37.470167Z",
     "iopub.status.idle": "2024-12-11T12:07:37.482890Z",
     "shell.execute_reply": "2024-12-11T12:07:37.481831Z"
    }
   },
   "outputs": [],
   "source": [
    "def assign_voting_power(data):\n",
    "    \n",
    "    # Calculate and assign voting power percentages based on active membership\n",
    "    sum_vp = data[data['voting_power'] > 1]['voting_power'].sum()\n",
    "    data['th_vp'] = data.apply(lambda row: (row['voting_power'] * 100) / sum_vp if row['voting_power'] > 1 else 0, axis=1)\n",
    "\n",
    "    # Define the councils and committees along with their membership columns\n",
    "    councils_and_committees = [\n",
    "        ('ch_member_r2', 'ch_vp_r2'),\n",
    "        ('ch_member_r3', 'ch_vp_r3'),\n",
    "        ('ch_member_r4', 'ch_vp_r4'),\n",
    "        ('ch_member_r5', 'ch_vp_r5'),\n",
    "        ('ch_member_r6', 'ch_vp_r6'),\n",
    "        ('gc_member_s3', 'gc_vp_s3'),\n",
    "        ('gc_member_s4', 'gc_vp_s4'),\n",
    "        ('gc_member_s5', 'gc_vp_s5'),\n",
    "        ('gc_member_mm_s5', 'gc_vp_mm_s5'),\n",
    "        ('dab_member_s5', 'dab_vp_s5'),\n",
    "        ('coc_member_s5', 'coc_vp_s5'),\n",
    "        ('gc_member_s6', 'gc_vp_s6'),\n",
    "        ('gc_member_mm_s6', 'gc_vp_mm_s6'),\n",
    "        ('dab_member_s6', 'dab_vp_s6'),\n",
    "        ('coc_member_s6', 'coc_vp_s6')\n",
    "    ]\n",
    "\n",
    "    # Loop through each council/committee to calculate the voting power percentage\n",
    "    for member_col, vp_col in councils_and_committees:\n",
    "        count_member = data[member_col].sum() \n",
    "        data[vp_col] = data.apply(\n",
    "            lambda row: (row[member_col] * 100) / count_member if row[member_col] == 1 else 0, axis=1\n",
    "        )\n",
    "\n",
    "    return data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Datasheet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-11T12:07:37.489854Z",
     "iopub.status.busy": "2024-12-11T12:07:37.489854Z",
     "iopub.status.idle": "2024-12-11T12:07:37.887957Z",
     "shell.execute_reply": "2024-12-11T12:07:37.885941Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import pymongo\n",
    "from pymongo import MongoClient\n",
    "\n",
    "def create_data_sheet(data, filename):\n",
    "    # MongoDB Connection Setup\n",
    "    # Replace with your actual MongoDB connection string\n",
    "    # Example format: 'mongodb://username:password@host:port/database'\n",
    "    MONGO_URI = 'mongodb://localhost:27017/'\n",
    "    DATABASE_NAME = 'delegates_db'\n",
    "    COLLECTION_NAME = 'data_sheets'\n",
    "\n",
    "    # Establish MongoDB Connection\n",
    "    try:\n",
    "        # Connect to MongoDB\n",
    "        client = MongoClient(MONGO_URI)\n",
    "        db = client[DATABASE_NAME]\n",
    "        collection = db[COLLECTION_NAME]\n",
    "\n",
    "        # Extract the date from the file name (assumed format 'YYYY-MM-DD.csv')\n",
    "        file_date = datetime.strptime(filename[:-4], '%Y-%m-%d')\n",
    "    \n",
    "        # Load data from other CSV files\n",
    "        citizens_round2 = pd.read_csv(\"./Data/Round 2.csv\", encoding='latin1')\n",
    "        citizens_round3 = pd.read_csv(\"./Data/Round 3.csv\", encoding='latin1')\n",
    "        citizens_round4 = pd.read_csv(\"./Data/Round 4.csv\", encoding='latin1')\n",
    "        citizens_round5 = pd.read_csv(\"./Data/Round 5.csv\", encoding='latin1')\n",
    "        citizens_round6 = pd.read_csv(\"./Data/Round 6.csv\", encoding='latin1')\n",
    "        grants = pd.read_csv(\"./Data/Grants_Council.csv\", encoding='latin1')\n",
    "        grants_mm = pd.read_csv(\"./Data/Grants_Council_MM.csv\", encoding='latin1')\n",
    "        dab = pd.read_csv(\"./Data/Developer_Advisory_Board.csv\", encoding='latin1')\n",
    "        coc = pd.read_csv(\"./Data/Code_of_Conduct_Council.csv\", encoding='latin1')\n",
    "\n",
    "        # Drop rows with null values in the 'address' column for each DataFrame\n",
    "        citizens_round2.dropna(subset=['address'], inplace=True)\n",
    "        citizens_round3.dropna(subset=['address'], inplace=True)\n",
    "        citizens_round4.dropna(subset=['address'], inplace=True)\n",
    "        citizens_round5.dropna(subset=['address'], inplace=True)\n",
    "        citizens_round6.dropna(subset=['address'], inplace=True)\n",
    "        grants.dropna(subset=['address'], inplace=True)\n",
    "        grants_mm.dropna(subset=['address'], inplace=True)\n",
    "        dab.dropna(subset=['address'], inplace=True)\n",
    "        coc.dropna(subset=['address'], inplace=True)\n",
    "\n",
    "        # Filter grants data by season \n",
    "        grants_season3 = grants[grants['season'] == 3]\n",
    "        grants_season4 = grants[grants['season'] == 4]\n",
    "        grants_season5 = grants[grants['season'] == 5]\n",
    "        grants_mm_season5 = grants_mm[grants_mm['season'] == 5]\n",
    "        dab_season5 = dab[dab[\"season\"] == 5]\n",
    "        coc_season5 = coc[coc[\"season\"] == 5]\n",
    "        grants_season6 = grants[grants['season'] == 6]\n",
    "        grants_mm_season6 = grants_mm[grants_mm['season'] == 6]\n",
    "        dab_season6 = dab[dab[\"season\"] == 6]\n",
    "        coc_season6 = coc[coc[\"season\"] == 6]\n",
    "\n",
    "        # Calling the function and capturing the returned data\n",
    "        citizens_round2 = convert_dates(citizens_round2)\n",
    "        citizens_round3 = convert_dates(citizens_round3)\n",
    "        citizens_round4 = convert_dates(citizens_round4)\n",
    "        citizens_round5 = convert_dates(citizens_round5)\n",
    "        citizens_round6 = convert_dates(citizens_round6)\n",
    "        grants_season3 = convert_dates(grants_season3)\n",
    "        grants_season4 = convert_dates(grants_season4)\n",
    "        grants_season5 = convert_dates(grants_season5)\n",
    "        grants_mm_season5 = convert_dates(grants_mm_season5)\n",
    "        dab_season5 = convert_dates(dab_season5)\n",
    "        coc_season5 = convert_dates(coc_season5)\n",
    "        grants_season6 = convert_dates(grants_season6)\n",
    "        grants_mm_season6 = convert_dates(grants_mm_season6)\n",
    "        dab_season6 = convert_dates(dab_season6)\n",
    "        coc_season6 = convert_dates(coc_season6)\n",
    "\n",
    "        # Add missing delegates from various rounds and councils\n",
    "        data = add_missing_delegates(data, citizens_round2)\n",
    "        data = add_missing_delegates(data, citizens_round3)\n",
    "        data = add_missing_delegates(data, citizens_round4)\n",
    "        data = add_missing_delegates(data, citizens_round5)\n",
    "        data = add_missing_delegates(data, citizens_round6)\n",
    "        data = add_missing_delegates(data, grants_season3)\n",
    "        data = add_missing_delegates(data, grants_season4)\n",
    "        data = add_missing_delegates(data, grants_season5)\n",
    "        data = add_missing_delegates(data, grants_mm_season5)\n",
    "        data = add_missing_delegates(data, dab_season5)\n",
    "        data = add_missing_delegates(data, coc_season5)\n",
    "        data = add_missing_delegates(data, grants_season6)\n",
    "        data = add_missing_delegates(data, grants_mm_season6)\n",
    "        data = add_missing_delegates(data, dab_season6)\n",
    "        data = add_missing_delegates(data, coc_season6)\n",
    "        \n",
    "        # Add the columns \n",
    "        data['th_vp'] = None\n",
    "        data['ch_member_r2'] = None\n",
    "        data['ch_vp_r2'] = None\n",
    "        data['ch_member_r3'] = None\n",
    "        data['ch_vp_r3'] = None\n",
    "        data['ch_member_r4'] = None\n",
    "        data['ch_vp_r4'] = None\n",
    "        data['ch_member_r5'] = None\n",
    "        data['ch_vp_r5'] = None\n",
    "        data['ch_member_r6'] = None\n",
    "        data['ch_vp_r6'] = None\n",
    "        data['gc_member_s3'] = None\n",
    "        data['gc_vp_s3'] = None\n",
    "        data['gc_member_s4'] = None\n",
    "        data['gc_vp_s4'] = None\n",
    "        data['gc_member_s5'] = None\n",
    "        data['gc_vp_s5'] = None\n",
    "        data['gc_member_mm_s5'] = None\n",
    "        data['gc_vp_mm_s5'] = None\n",
    "        data['sc_member_s5'] = None\n",
    "        data['sc_vp_s5'] = None\n",
    "        data['coc_member_s5'] = None\n",
    "        data['coc_vp_s5'] = None\n",
    "        data['dab_member_s5'] = None\n",
    "        data['dab_vp_s5'] = None\n",
    "        data['gc_member_s6'] = None\n",
    "        data['gc_vp_s6'] = None\n",
    "        data['gc_member_mm_s6'] = None\n",
    "        data['gc_vp_mm_s6'] = None\n",
    "        data['sc_member_s6'] = None\n",
    "        data['sc_vp_s6'] = None\n",
    "        data['coc_member_s6'] = None\n",
    "        data['coc_vp_s6'] = None\n",
    "        data['dab_member_s6'] = None\n",
    "        data['dab_vp_s6'] = None\n",
    "\n",
    "        councils = [\n",
    "        ('ch_member_r2', citizens_round2),\n",
    "        ('ch_member_r3', citizens_round3),\n",
    "        ('ch_member_r4', citizens_round4),\n",
    "        ('ch_member_r5', citizens_round5),\n",
    "        ('ch_member_r6', citizens_round6),\n",
    "        ('gc_member_s3', grants_season3),\n",
    "        ('gc_member_s4', grants_season4),\n",
    "        ('gc_member_s5', grants_season5),\n",
    "        ('gc_member_mm_s5', grants_mm_season5),\n",
    "        ('dab_member_s5', dab_season5),\n",
    "        ('coc_member_s5', coc_season5),\n",
    "        ('gc_member_s6', grants_season6),\n",
    "        ('gc_member_mm_s6', grants_mm_season6),\n",
    "        ('dab_member_s6', dab_season6),\n",
    "        ('coc_member_s6', coc_season6)\n",
    "    ]\n",
    "\n",
    "        # Add membership columns dynamically\n",
    "        data = add_membership_columns(data, councils, file_date)\n",
    "\n",
    "        data = assign_voting_power(data)\n",
    "\n",
    "        # Fill all null values in the dataframe with 0\n",
    "        data.fillna(0, inplace=True)\n",
    "        \n",
    "        # try:\n",
    "        # Prepare records in the specified structure\n",
    "        records = []\n",
    "        current_time = datetime.utcnow()\n",
    "\n",
    "        for _, row in data.iterrows():\n",
    "            # Extract voting power fields\n",
    "            voting_power = {}\n",
    "            \n",
    "            # Add total voting power if exists\n",
    "            if row.get('voting_power') is not None and row['voting_power'] != 0:\n",
    "                voting_power['vp'] = row['voting_power']\n",
    "            \n",
    "            # Add TH voting power if exists\n",
    "            if row.get('th_vp') is not None and row['th_vp'] != 0:\n",
    "                voting_power['th_vp'] = row['th_vp']\n",
    "            \n",
    "            # Add various round and season voting power fields\n",
    "            vp_columns = [\n",
    "                'ch_vp_r2', 'ch_vp_r3', 'ch_vp_r4', 'ch_vp_r5', 'ch_vp_r6',\n",
    "                'gc_vp_s3', 'gc_vp_s4', 'gc_vp_s5', 'gc_vp_mm_s5',\n",
    "                'gc_vp_s6', 'gc_vp_mm_s6',\n",
    "                'sc_vp_s5', 'sc_vp_s6',\n",
    "                'coc_vp_s5', 'coc_vp_s6',\n",
    "                'dab_vp_s5', 'dab_vp_s6'\n",
    "            ]\n",
    "            \n",
    "            for col in vp_columns:\n",
    "                if row.get(col) is not None and row[col] != 0:\n",
    "                    voting_power[col] = row[col]\n",
    "            \n",
    "            # Only create a record if there's voting power\n",
    "            if voting_power:\n",
    "                record = {\n",
    "                    \"date\": file_date,  # Use the date from the filename\n",
    "                    \"delegate_id\": row['delegate'],  # Assuming 'delegate' column exists\n",
    "                    \"voting_power\": voting_power,\n",
    "                    \"updatedAt\": current_time\n",
    "                }\n",
    "                records.append(record)\n",
    "\n",
    "        # Insert records into MongoDB\n",
    "        if records:\n",
    "            # Use insert_many with ordered=False to continue insertion even if some documents fail\n",
    "            inserted_count = 0\n",
    "            updated_count = 0\n",
    "    \n",
    "            for record in records:\n",
    "                # Use upsert=True to update the document if it exists, otherwise insert a new one\n",
    "                result = collection.update_one(\n",
    "                    {\"delegate_id\": record[\"delegate_id\"]},  # Match delegate_id \n",
    "                    {\"$set\": record},  # Overwrite the data\n",
    "                    upsert=True\n",
    "                )\n",
    "        \n",
    "            if result.upserted_id is not None:\n",
    "                inserted_count += 1  # Count new insertions\n",
    "            elif result.matched_count > 0:\n",
    "                updated_count += 1  # Count updates            \n",
    "                print(f\"Data from {filename} saved to MongoDB successfully!\")\n",
    "                # print(f\"Inserted {len(result.inserted_ids)} records\")\n",
    "        \n",
    "        return data\n",
    "\n",
    "    except pymongo.errors.BulkWriteError as e:\n",
    "        # Handle potential duplicate key errors\n",
    "        print(\"Partial insertion occurred:\")\n",
    "        print(f\"Inserted: {len(e.details['writeErrors'])}\")\n",
    "        print(f\"Errors: {e.details['writeErrors']}\")\n",
    "        return data\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred during MongoDB insertion: {e}\")\n",
    "        return None\n",
    "    finally:\n",
    "        # Close the MongoDB connection\n",
    "        client.close()\n",
    "        # Prepare data for MongoDB insertion\n",
    "        # Convert DataFrame to list of dictionaries\n",
    "    #     records = data.to_dict('records')\n",
    "\n",
    "    #     # Add filename and date to each record for tracking\n",
    "    #     for record in records:\n",
    "    #         record['filename'] = filename\n",
    "    #         record['import_date'] = file_date\n",
    "\n",
    "    #     # Insert records into MongoDB\n",
    "    #     if records:\n",
    "    #         collection.insert_many(records)\n",
    "    #         print(f\"Data from {filename} saved to MongoDB successfully!\")\n",
    "\n",
    "    #     return data\n",
    "\n",
    "    # except pymongo.errors.ConnectionFailure as e:\n",
    "    #     print(f\"Failed to connect to MongoDB: {e}\")\n",
    "    #     return None\n",
    "    # except Exception as e:\n",
    "    #     print(f\"An error occurred: {e}\")\n",
    "    #     return None\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate HHI and CPI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-11T12:07:37.894954Z",
     "iopub.status.busy": "2024-12-11T12:07:37.893956Z",
     "iopub.status.idle": "2024-12-11T12:07:37.918443Z",
     "shell.execute_reply": "2024-12-11T12:07:37.916190Z"
    }
   },
   "outputs": [],
   "source": [
    "# Define each influence period with start_date, end_date, and influence percentages\n",
    "influence_periods = [\n",
    "    # May 26th, 2022 - January 25th, 2023\n",
    "    {\n",
    "        \"start_date\": \"2022-05-26\", \"end_date\": \"2023-01-25\",\n",
    "        \"influences\": {\"th_vp\": 48.32, \"ch_vp_r2\": 51.68}\n",
    "    },\n",
    "    # January 26th, 2023 - March 30th, 2023\n",
    "    {\n",
    "        \"start_date\": \"2023-01-26\", \"end_date\": \"2023-03-30\",\n",
    "        \"influences\": {\"th_vp\": 41.95, \"ch_vp_r2\": 44.88, \"gc_vp_s3\": 13.17}\n",
    "    },\n",
    "    # March 31st, 2023 - June 7th, 2023\n",
    "    {\n",
    "        \"start_date\": \"2023-03-31\", \"end_date\": \"2023-06-07\",\n",
    "        \"influences\": {\"th_vp\": 41.95, \"ch_vp_r3\": 44.88, \"gc_vp_s3\": 13.17}\n",
    "    },\n",
    "    # June 8th, 2023 - January 3rd, 2024\n",
    "    {\n",
    "        \"start_date\": \"2023-06-08\", \"end_date\": \"2024-01-03\",\n",
    "        \"influences\": {\"th_vp\": 41.95, \"ch_vp_r3\": 44.88, \"gc_vp_s4\": 13.17}\n",
    "    },\n",
    "    # January 4th, 2024 - January 11th, 2024\n",
    "    {\n",
    "        \"start_date\": \"2024-01-04\", \"end_date\": \"2024-01-11\",\n",
    "        \"influences\": {\n",
    "            \"th_vp\": 32.33, \"ch_vp_r3\": 34.59, \"gc_vp_s5\": 10.15,\n",
    "            \"gc_vp_mm_s5\": 2.82, \"sc_vp_s5\": 12.78, \"coc_vp_s5\": 4.32,\n",
    "            \"dab_vp_s5\": 3.01\n",
    "        }\n",
    "    },\n",
    "    # January 12th, 2024 - June 26th, 2024\n",
    "    {\n",
    "        \"start_date\": \"2024-01-12\", \"end_date\": \"2024-06-26\",\n",
    "        \"influences\": {\n",
    "            \"th_vp\": 32.33, \"ch_vp_r4\": 34.59, \"gc_vp_s5\": 10.15,\n",
    "            \"gc_vp_mm_s5\": 2.82, \"sc_vp_s5\": 12.78, \"coc_vp_s5\": 4.32,\n",
    "            \"dab_vp_s5\": 3.01\n",
    "        }\n",
    "    },\n",
    "    # June 27th, 2024 - July 16th, 2024\n",
    "    {\n",
    "        \"start_date\": \"2024-06-27\", \"end_date\": \"2024-07-16\",\n",
    "        \"influences\": {\n",
    "            \"th_vp\": 32.33, \"ch_vp_r4\": 34.59, \"gc_vp_s6\": 10.15,\n",
    "            \"gc_vp_mm_s6\": 2.82, \"sc_vp_s6\": 12.78, \"coc_vp_s6\": 4.32,\n",
    "            \"dab_vp_s6\": 3.01\n",
    "        }\n",
    "    },\n",
    "    # July 17th, 2024 - October 21st, 2024\n",
    "    {\n",
    "        \"start_date\": \"2024-07-17\", \"end_date\": \"2024-10-21\",\n",
    "        \"influences\": {\n",
    "            \"th_vp\": 32.33, \"ch_vp_r5\": 34.59, \"gc_vp_s6\": 10.15,\n",
    "            \"gc_vp_mm_s6\": 2.82, \"sc_vp_s6\": 12.78, \"coc_vp_s6\": 4.32,\n",
    "            \"dab_vp_s6\": 3.01\n",
    "        }\n",
    "    },\n",
    "    # October 22nd, 2024 - December 31st, 2024\n",
    "    {\n",
    "        \"start_date\": \"2024-10-22\", \"end_date\": \"2024-12-31\",\n",
    "        \"influences\": {\n",
    "            \"th_vp\": 32.33, \"ch_vp_r6\": 34.59, \"gc_vp_s6\": 10.15,\n",
    "            \"gc_vp_mm_s6\": 2.82, \"sc_vp_s6\": 12.78, \"coc_vp_s6\": 4.32,\n",
    "            \"dab_vp_s6\": 3.01\n",
    "        }\n",
    "    }\n",
    "    # # October 22nd, 2024 - December 2nd, 2024\n",
    "    # {\n",
    "    #     \"start_date\": \"2024-10-22\", \"end_date\": \"2024-12-02\",\n",
    "    #     \"influences\": {\n",
    "    #         \"th_vp\": 32.33, \"ch_vp_r6\": 34.59, \"gc_vp_s6\": 10.15,\n",
    "    #         \"gc_vp_mm_s6\": 2.82, \"sc_vp_s6\": 12.78, \"coc_vp_s6\": 4.32,\n",
    "    #         \"dab_vp_s6\": 3.01\n",
    "    #     }\n",
    "    # },\n",
    "    # # December 3rd, 2024 - December 11th, 2024\n",
    "    # {\n",
    "    #     \"start_date\": \"2024-12-03\", \"end_date\": \"2024-12-11\",\n",
    "    #     \"influences\": {\n",
    "    #         \"th_vp\": 32.33, \"ch_vp_r6\": 34.59, \"gc_vp_s6\": 10.15,\n",
    "    #         \"gc_vp_mm_s6\": 2.82, \"sc_vp_s6\": 12.78, \"coc_vp_s6\": 4.32,\n",
    "    #         \"dab_vp_s6\": 3.01\n",
    "    #     }\n",
    "    # }\n",
    "]\n",
    "\n",
    "def calculate_influence(row, influences):\n",
    "    \"\"\"Calculates influence based on influence percentages per column.\"\"\"\n",
    "    influence_sum = sum(row.get(col, 0) * (val / 100) for col, val in influences.items())\n",
    "    return influence_sum\n",
    "\n",
    "def add_influence_column(df, file_date_str):\n",
    "    \"\"\"Adds 'influence' column to DataFrame based on file date.\"\"\"\n",
    "    file_date = datetime.strptime(file_date_str, \"%Y-%m-%d\")\n",
    "    for period in influence_periods:\n",
    "        start_date = datetime.strptime(period[\"start_date\"], \"%Y-%m-%d\")\n",
    "        end_date = datetime.strptime(period[\"end_date\"], \"%Y-%m-%d\")\n",
    "        if start_date <= file_date <= end_date:\n",
    "            df[\"influence\"] = df.apply(calculate_influence, axis=1, influences=period[\"influences\"])\n",
    "            break\n",
    "    return df\n",
    "\n",
    "def calculate_HHI_and_CPI(data, file_date_str):\n",
    "    \"\"\"Calculate HHI and CPI based on the data.\"\"\"\n",
    "    print(f\"Calculating HHI and CPI for {file_date_str}...\")\n",
    "    data = add_influence_column(data, file_date_str)\n",
    "    data['th_vp_squared'] = data['th_vp'] ** 2\n",
    "    data['influence_squared'] = data['influence'] ** 2\n",
    "    HHI = round(data['th_vp_squared'].sum(), 2)\n",
    "    CPI = round(data['influence_squared'].sum(), 2)\n",
    "\n",
    "    print(f\"Date: {file_date_str} | HHI: {HHI} | CPI: {CPI}\")\n",
    "    return HHI, CPI\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-11T12:07:37.926210Z",
     "iopub.status.busy": "2024-12-11T12:07:37.925210Z",
     "iopub.status.idle": "2024-12-11T12:08:28.807787Z",
     "shell.execute_reply": "2024-12-11T12:08:28.806773Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data saved to ./Data/2024-12-11.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_17436\\521277835.py:154: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  data.fillna(0, inplace=True)\n",
      "C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_17436\\521277835.py:159: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  current_time = datetime.utcnow()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data from 2024-12-11.csv saved to MongoDB successfully!\n",
      "Calculating HHI and CPI for 2024-12-11...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Date: 2024-12-11 | HHI: 324.39 | CPI: 74.99\n"
     ]
    }
   ],
   "source": [
    "data, filename = fetch_top_5000_delegates()\n",
    "\n",
    "data = create_data_sheet(data, filename)\n",
    "\n",
    "file_date_str = os.path.splitext(filename)[0] \n",
    "hhi, cpi = calculate_HHI_and_CPI(data, file_date_str)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
